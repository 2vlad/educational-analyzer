import { env } from '@/src/config/env'
import { modelsManager } from '@/src/config/models'
import { logger } from '@/src/utils/logger'
import {
  getPrompt,
  getProviderFamily,
  fillPromptTemplate,
  getPromptSnippet,
  type Metric,
} from '@/src/utils/prompts'
import { ClaudeProvider } from '@/src/providers/claude'
import { OpenAIProvider } from '@/src/providers/openai'
import { GeminiProvider } from '@/src/providers/gemini'
import { YandexProvider } from '@/src/providers/yandex'
import { LLMProvider, GenerateResult, ProviderError } from '@/src/providers/types'

export class LLMService {
  private providers: Map<string, LLMProvider> = new Map()
  private currentProviderId: string

  constructor(providerId?: string) {
    this.currentProviderId = providerId || modelsManager.getDefaultModel()
    this.initializeProviders()
  }

  private initializeProviders() {
    console.log('üîß Initializing LLM providers...')
    console.log('Environment check:')
    console.log('- env.isServer:', env.isServer)
    console.log('- env.server exists:', !!env.server)

    // Initialize available providers based on API keys
    if (env.isServer && env.server) {
      if (env.server.ANTHROPIC_API_KEY) {
        console.log('‚úÖ Initializing Anthropic provider')
        this.providers.set('anthropic', new ClaudeProvider())
      } else {
        console.log('‚ö†Ô∏è Anthropic API key not found')
      }

      if (env.server.OPENAI_API_KEY) {
        console.log('‚úÖ Initializing OpenAI provider')
        this.providers.set('openai', new OpenAIProvider())
      } else {
        console.log('‚ö†Ô∏è OpenAI API key not found')
      }

      if (env.server.GOOGLE_API_KEY) {
        console.log('‚úÖ Initializing Google provider')
        this.providers.set('google', new GeminiProvider())
      } else {
        console.log('‚ö†Ô∏è Google API key not found')
      }

      if (env.server.YANDEX_API_KEY && env.server.YANDEX_FOLDER_ID) {
        console.log('‚úÖ Initializing Yandex provider')
        console.log('  - API Key length:', env.server.YANDEX_API_KEY.length)
        console.log('  - Folder ID:', env.server.YANDEX_FOLDER_ID)
        this.providers.set('yandex', new YandexProvider())
      } else {
        console.log('‚ö†Ô∏è Yandex provider not initialized:')
        console.log('  - API Key:', env.server?.YANDEX_API_KEY ? 'SET' : 'NOT SET')
        console.log('  - Folder ID:', env.server?.YANDEX_FOLDER_ID ? 'SET' : 'NOT SET')
      }
    } else {
      console.error('‚ùå Not on server or env.server is not available!')
    }

    console.log('ü§ñ LLMService initialized with providers:', Array.from(this.providers.keys()))

    if (this.providers.size === 0) {
      console.error('‚ùå CRITICAL: No LLM providers available! Analysis will fail.')
    }
  }

  public getProvider(providerId: string): LLMProvider {
    const modelConfig = modelsManager.getModelConfig(providerId)
    if (!modelConfig) {
      throw new Error(`Unknown model: ${providerId}`)
    }

    const provider = this.providers.get(modelConfig.provider)
    if (!provider) {
      throw new Error(`Provider not available: ${modelConfig.provider}`)
    }

    return provider
  }

  async analyze(
    content: string,
    metric: Metric,
    customPromptText?: string,
  ): Promise<GenerateResult> {
    const modelConfig = modelsManager.getModelConfig(this.currentProviderId)
    if (!modelConfig) {
      throw new Error(`Model configuration not found: ${this.currentProviderId}`)
    }

    // Get prompt for the provider family
    const providerFamily = getProviderFamily(this.currentProviderId)
    // Use custom prompt text if provided, otherwise load from file
    let prompt = customPromptText || getPrompt(providerFamily, metric)

    // For custom prompts, ensure JSON format for proper parsing
    if (customPromptText && !customPromptText.includes('json')) {
      prompt = `${customPromptText}

–í–ê–ñ–ù–û: –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º:
\`\`\`json
{
  "score": -2|-1|0|1|2,
  "comment": "–∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–º–∞–∫—Å 150 —Å–∏–º–≤–æ–ª–æ–≤)",
  "examples": [
    "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏–ª–ª—é—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–∞—à—É –æ—Ü–µ–Ω–∫—É",
    "–ï—â–µ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"
  ],
  "detailed_analysis": "–ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –ø–æ –¥–∞–Ω–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
  "suggestions": [
    "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é",
    "–ï—â–µ –æ–¥–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"
  ]
}
\`\`\`

–ú–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{{content}}`
    }

    const filledPrompt = fillPromptTemplate(prompt, content)

    console.log('\nüìù LLMService.analyze()')
    console.log('Metric:', metric)
    console.log('Model:', this.currentProviderId)
    console.log('Provider:', providerFamily)
    console.log('Content length:', content.length)
    console.log('Prompt length:', filledPrompt.length)

    // Log request start
    const analysisId = globalThis.crypto.randomUUID()
    logger.llmRequestStart({
      analysisId,
      metric,
      model: this.currentProviderId,
      promptLength: filledPrompt.length,
      contentLength: content.length,
    })

    try {
      // Get provider and generate
      const provider = this.getProvider(this.currentProviderId)
      const result = await provider.generate(prompt, content, {
        model: modelConfig.model,
        temperature: modelConfig.temperature,
        maxTokens: modelConfig.maxTokens,
        timeoutMs: env.server?.REQUEST_TIMEOUT || 30000,
      })

      // Log success
      logger.llmRequestComplete({
        analysisId,
        metric,
        model: this.currentProviderId,
        duration: result.durationMs,
        tokensUsed: result.tokensUsed,
        success: true,
      })

      return result
    } catch (error) {
      // Log error
      logger.llmRequestError({
        analysisId,
        metric,
        model: this.currentProviderId,
        error: error instanceof Error ? error.message : 'Unknown error',
        retryCount: 0,
        promptSnippet: getPromptSnippet(filledPrompt),
      })

      throw error
    }
  }

  async analyzeWithRetry(
    content: string,
    metric: Metric,
    maxRetries?: number,
    customPromptText?: string,
  ): Promise<GenerateResult> {
    const retries = maxRetries || env.server?.MAX_RETRIES || 3
    let lastError: Error | undefined

    for (let attempt = 1; attempt <= retries; attempt++) {
      try {
        const result = await this.analyze(content, metric, customPromptText)

        if (attempt > 1) {
          logger.llmSuccess({ metric, attempt })
        }

        return result
      } catch (error) {
        lastError = error as Error

        // Check if error is retryable
        if (error instanceof ProviderError && !error.retryable) {
          throw error
        }

        logger.llmRetry({
          metric,
          attempt,
          error: lastError.message,
        })

        // Add exponential backoff delay before retry (except on last attempt)
        if (attempt < retries) {
          const delay = Math.min(1000 * Math.pow(2, attempt - 1), 10000) // 1s, 2s, 4s, max 10s
          console.log(`‚è≥ Waiting ${delay}ms before retry attempt ${attempt + 1}/${retries}...`)
          await new Promise((resolve) => globalThis.setTimeout(resolve, delay))
        }

        // If this was the last attempt and model switching is enabled, try fallback
        if (attempt === retries && modelsManager.isModelSwitchingEnabled()) {
          const fallbackModel = modelsManager.getNextFallbackModel(this.currentProviderId)

          if (fallbackModel) {
            logger.modelFallback({ metric, fallbackModel })

            // Switch to fallback model and try once more
            const oldModel = this.currentProviderId
            this.currentProviderId = fallbackModel

            logger.modelSwitch({
              from: oldModel,
              to: fallbackModel,
              reason: 'Max retries reached',
            })

            try {
              return await this.analyze(content, metric, customPromptText)
            } catch (fallbackError) {
              // Restore original model
              this.currentProviderId = oldModel
              throw fallbackError
            }
          }
        }
      }
    }

    throw lastError || new Error('Max retries reached')
  }

  async generateTitle(content: string, providerId?: string): Promise<GenerateResult> {
    const modelId = providerId || this.currentProviderId
    const modelConfig = modelsManager.getModelConfig(modelId)
    if (!modelConfig) {
      throw new Error(`Model configuration not found: ${modelId}`)
    }

    const titlePrompt = `–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –∏ –¥–∞–π –µ–º—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (3-6 —Å–ª–æ–≤), –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ç–µ–º—É. 
–ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Å–ª–æ–≤ —Ç–∏–ø–∞ "–û—Å–Ω–æ–≤—ã", "–í–≤–µ–¥–µ–Ω–∏–µ", "–£—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª". 
–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –º–µ—Ç–æ–¥—ã –∏–ª–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.

–ú–∞—Ç–µ—Ä–∏–∞–ª:
${content.substring(0, 1500)}...

–ù–∞–∑–≤–∞–Ω–∏–µ:`

    try {
      const provider = this.getProvider(modelId)
      const result = await provider.generate(titlePrompt, '', {
        model: modelConfig.model,
        temperature: 0.3,
        maxTokens: 50,
        timeoutMs: 5000,
      })

      // Clean up the title
      if (result.comment) {
        result.comment = result.comment.trim().replace(/["`']/g, '').substring(0, 100)
      }

      return result
    } catch (error) {
      console.error('Failed to generate title:', error)
      return {
        score: 0,
        comment: '–£—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª',
        examples: [],
        model: modelId,
        durationMs: 0,
        raw: null,
        provider: 'unknown',
      }
    }
  }

  async analyzeWithModel(
    content: string,
    metric: Metric,
    providerId: string,
    customPromptText?: string,
  ): Promise<GenerateResult> {
    const oldModel = this.currentProviderId
    this.currentProviderId = providerId

    try {
      logger.modelSwitch({
        from: oldModel,
        to: providerId,
        reason: 'Explicit model selection',
      })

      return await this.analyze(content, metric, customPromptText)
    } finally {
      this.currentProviderId = oldModel
    }
  }

  getCurrentModel(): string {
    return this.currentProviderId
  }

  getAvailableModels(): string[] {
    return modelsManager.getAvailableModels()
  }

  /**
   * Analyze coherence and connections between multiple lessons
   * @param lessons Array of lessons with titles and content
   * @param providerId Optional specific model to use
   * @returns Analysis of lesson coherence
   */
  async analyzeCoherence(
    lessons: Array<{ title: string; content: string }>,
    providerId?: string,
  ): Promise<{
    score: number
    summary: string
    strengths: string[]
    issues: string[]
    suggestions: string[]
  }> {
    const modelId = providerId || this.currentProviderId
    const modelConfig = modelsManager.getModelConfig(modelId)
    if (!modelConfig) {
      throw new Error(`Model configuration not found: ${modelId}`)
    }

    // Create a compact representation of lessons for analysis
    const lessonsOverview = lessons
      .map((lesson, i) => {
        // Take first 500 chars of each lesson for context
        const preview = lesson.content.substring(0, 500)
        return `${i + 1}. "${lesson.title}"\n${preview}...`
      })
      .join('\n\n')

    const coherencePrompt = `–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —É—á–µ–±–Ω—ã–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–≤—è–∑–Ω–æ—Å—Ç—å –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É—Ä–æ–∫–æ–≤.

–£—Ä–æ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
${lessonsOverview}

–û—Ü–µ–Ω–∏ –ø–æ —à–∫–∞–ª–µ –æ—Ç -2 –¥–æ +2:
- -2: –£—Ä–æ–∫–∏ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ —Å–≤—è–∑–∞–Ω—ã, —Ö–∞–æ—Ç–∏—á–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- -1: –°–ª–∞–±–∞—è —Å–≤—è–∑—å, –µ—Å—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã
- 0: –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è —Å–≤—è–∑—å, –º–∞—Ç–µ—Ä–∏–∞–ª —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã–π –Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–π
- +1: –•–æ—Ä–æ—à–∞—è —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å, –ª–æ–≥–∏—á–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- +2: –û—Ç–ª–∏—á–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å, –∫–∞–∂–¥—ã–π —É—Ä–æ–∫ –ø–ª–∞–≤–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–π

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
\`\`\`json
{
  "score": -2|-1|0|1|2,
  "summary": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—â–µ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
  "strengths": ["–°–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 1", "–°–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 2"],
  "issues": ["–ü—Ä–æ–±–ª–µ–º–∞ 1", "–ü—Ä–æ–±–ª–µ–º–∞ 2"],
  "suggestions": ["–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1", "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2"]
}
\`\`\``

    try {
      const provider = this.getProvider(modelId)
      const result = await provider.generate(coherencePrompt, '', {
        model: modelConfig.model,
        temperature: 0.3,
        maxTokens: 1000,
        timeoutMs: 20000,
      })

      // Parse the response
      let parsed: {
        score?: number
        summary?: string
        strengths?: string[]
        issues?: string[]
        suggestions?: string[]
      }
      try {
        // Try to extract JSON from the response
        const jsonMatch = result.comment?.match(/\{[\s\S]*\}/)
        if (jsonMatch) {
          parsed = JSON.parse(jsonMatch[0])
        } else {
          throw new Error('No JSON found in response')
        }
      } catch (parseError) {
        console.error('Failed to parse coherence analysis:', parseError)
        // Return default structure
        return {
          score: 0,
          summary: result.comment || '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–Ω–æ—Å—Ç—å —É—Ä–æ–∫–æ–≤',
          strengths: [],
          issues: [],
          suggestions: [],
        }
      }

      return {
        score: parsed.score || 0,
        summary: parsed.summary || '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω',
        strengths: Array.isArray(parsed.strengths) ? parsed.strengths : [],
        issues: Array.isArray(parsed.issues) ? parsed.issues : [],
        suggestions: Array.isArray(parsed.suggestions) ? parsed.suggestions : [],
      }
    } catch (error) {
      console.error('Failed to analyze coherence:', error)
      return {
        score: 0,
        summary: '–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ —É—Ä–æ–∫–æ–≤',
        strengths: [],
        issues: [],
        suggestions: [],
      }
    }
  }
}

// Export singleton for convenience
export const llmService = new LLMService()
